# robots.txt - Search Engine Crawler Instructions
# This file tells search engines (Google, Bing, etc.) how to crawl your site

# Allow all search engine crawlers to access the site
# User-agent: * means "this applies to ALL crawlers"
User-agent: *

# Allow crawling of all public pages
# The / means "allow everything by default"
Allow: /

# Disallow (block) crawlers from accessing private/admin areas
# These pages shouldn't appear in search results
Disallow: /dashboard
Disallow: /api/

# Tell crawlers where to find your sitemap
# This helps them discover all your pages efficiently
Sitemap: https://ahmedlotfy.site/sitemap.xml

# Optional: Crawl-delay prevents overwhelming your server
# Crawl-delay: 10
